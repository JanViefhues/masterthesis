{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa864852",
   "metadata": {},
   "source": [
    "## Build the model and apply hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa046223",
   "metadata": {},
   "source": [
    "# 1. RF for *Flood* Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3a546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set and relevant libraries\n",
    "import pandas as pd\n",
    "\n",
    "total_dataset = pd.read_csv('/Users/jan-philippviefhues/Desktop/UNI/Maastricht/um/Thesis/gitRepo/masterthesis/cleaned_data/total_dataset_with_precipitation_and_dummies_hourly.csv')\n",
    "total_dataset = total_dataset.rename(columns={'PM25':'PM2.5', 'IsFloodingPeriode': 'Is flooding periode','period': 'Periode','RhineWaterLevel':'Rhine water level',\n",
    "                  'mine_water_level':'Mine water level', 'ground_water_level':'Ground water level','Stream_water_level':'Stream water level','abnormal_Co2_leakage':'CO2 leakage'})\n",
    "\n",
    "# Flood\n",
    "\n",
    "# Set features and target\n",
    "target = 'Is flooding periode' # Can be any kind of data type\n",
    "features = ['CO2','CO2 leakage','PM2.5', 'PM10', 'Temperature', 'Humidity', 'Pressure', 'Precipitation','Mine water level', 'Ground water level','Rhine water level','Stream water level','Discharge'] # Has to be an array\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76964110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow literature advice and use 10-fold cross validation to avoid for overfitting\n",
    "\n",
    "\n",
    "def calculate_rf_model(dataset, target, features, best_parameters):\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from pprint import pprint\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    " \n",
    "\n",
    "\n",
    "    X=dataset[features] # Has to be an array]  # Features\n",
    "    y=dataset[target]  # Labels\n",
    "\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11) # 80% training and 20% test\n",
    "    \n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    clf=RandomForestClassifier(n_estimators=best_parameters['n_estimators'],min_samples_split=best_parameters['min_samples_split'],\n",
    "                               min_samples_leaf=best_parameters['min_samples_leaf'], max_features=best_parameters['max_features'], \n",
    "                               max_depth=best_parameters['max_depth'],bootstrap= best_parameters['bootstrap'])\n",
    "     #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred=clf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # Outcome\n",
    "\n",
    "\n",
    "    \n",
    "    #Outcome\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    \n",
    "    # Calculate Feature Importance\n",
    "    featue_names = list(X.columns)\n",
    "    feature_imp = pd.Series(clf.feature_importances_,featue_names).sort_values(ascending=False)\n",
    "    feature_imp\n",
    "    %matplotlib inline\n",
    "    # Creating a bar plot\n",
    "    sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "    # Add labels to your graph\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Visualizing Important Features')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    feature_importances = clf.feature_importances_\n",
    "    sorted_importances = [importance for importance in feature_importances]\n",
    "    sorted_features = [importance for importance in feature_importances]\n",
    "    # Cumulative importances\n",
    "    cumulative_importances = np.cumsum(sorted_importances)\n",
    "    # Make a line graph\n",
    " \n",
    "    #print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)\n",
    "    print(sorted_features)\n",
    "        \n",
    "    \n",
    "    # Score gives accuracy\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    #accuracy = cross_val_score(clf, dataset[features], dataset[target], scoring='accuracy', cv = 10).mean() * 100\n",
    "    \n",
    "    #print(\"Accuracy of Random Forests is: \" , accuracy)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "    y_pred = cross_val_predict(clf, dataset[features], dataset[target], cv=10)\n",
    "\n",
    "    \n",
    "    labels = np.unique(y)\n",
    "    a =  confusion_matrix(y, y_pred, labels=labels)\n",
    "    \n",
    "    print(y_pred)\n",
    "\n",
    "    print(pd.DataFrame(a, index=labels, columns=labels))\n",
    "    \n",
    "\n",
    "    \n",
    "   # precision, recall and F1\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "\n",
    "    recall = cross_val_score(clf, dataset[features], dataset[target], cv=10, scoring='recall')\n",
    "    print('Recall', np.mean(recall), recall)\n",
    "    precision = cross_val_score(clf, dataset[features], dataset[target], cv=10, scoring='precision')\n",
    "    print('Precision', np.mean(precision), precision)\n",
    "    f1 = cross_val_score(clf, dataset[features], dataset[target], cv=10, scoring='f1')\n",
    "    print('F1', np.mean(f1), f1)\n",
    "    \n",
    "    \n",
    "   \n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c83a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_parameter_for_flood = {'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': True}\n",
    "calculate_rf_model(total_dataset, target, features,best_parameter_for_flood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cdb279",
   "metadata": {},
   "source": [
    "### Second interation with 8 features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2f6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 3 most important features\n",
    "eigth_features = ['Temperature', 'Humidity', 'Pressure','Mine water level', 'Ground water level','Rhine water level','Stream water level','Discharge'] # Has to be an array\n",
    "\n",
    "best_parameter_for_flood = {'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': True}\n",
    "calculate_rf_model(total_dataset, target, eigth_features,best_parameter_for_flood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9518f",
   "metadata": {},
   "source": [
    "# 2. RF for *CO2 Leakage* Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158a606",
   "metadata": {},
   "source": [
    "### BEFORE Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e8664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_before = pd.read_csv('/Users/jan-philippviefhues/Desktop/UNI/Maastricht/um/Thesis/data/datasets/cleaned_datasets/hourly_dataset.before.flooding_with_precipitation_and_dummies.csv')\n",
    "dataset_before = dataset_before.rename(columns={'PM25':'PM2.5', 'IsFloodingPeriode': 'Is flooding periode','period': 'Periode','RhineWaterLevel':'Rhine water level',\n",
    "                 'mine_water_level':'Mine water level', 'ground_water_level':'Ground water level','Stream_water_level':'Stream water level','abnormal_Co2_leakage':'CO2 leakage'})\n",
    "\n",
    "best_parameters_for_before_dataset = {'n_estimators': 1600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'bootstrap': True}\n",
    "target_before = 'CO2 leakage' # Can be any kind of data type\n",
    "features_before = ['Is flooding periode','PM2.5', 'PM10', 'Temperature', 'Humidity', 'Pressure', 'Precipitation','Mine water level', 'Ground water level','Rhine water level','Stream water level','Discharge'] # Has to be an array\n",
    "\n",
    "\n",
    "calculate_rf_model(dataset_before, target_before, features_before,best_parameters_for_before_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3f284",
   "metadata": {},
   "source": [
    "### DURING Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e65f44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_during = pd.read_csv('/Users/jan-philippviefhues/Desktop/UNI/Maastricht/um/Thesis/data/datasets/cleaned_datasets/hourly_dataset.during.flooding_with_precipitation_and_dummies.csv')\n",
    "dataset_during = dataset_during.rename(columns={'PM25':'PM2.5', 'IsFloodingPeriode': 'Is flooding periode','period': 'Periode','RhineWaterLevel':'Rhine water level',\n",
    "                  'mine_water_level':'Mine water level', 'ground_water_level':'Ground water level','Stream_water_level':'Stream water level','abnormal_Co2_leakage':'CO2 leakage'})\n",
    "\n",
    "best_parameters_for_during_dataset = {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 70, 'bootstrap': True}\n",
    "target_during = 'CO2 leakage' # Can be any kind of data type\n",
    "features_during = ['Is flooding periode','PM2.5', 'PM10', 'Temperature', 'Humidity', 'Pressure', 'Precipitation','Mine water level', 'Ground water level','Rhine water level','Stream water level','Discharge'] # Has to be an array\n",
    "calculate_rf_model(dataset_during, target_during, features_during,best_parameters_for_during_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9f556",
   "metadata": {},
   "source": [
    "### AFTER Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d35b55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_after = pd.read_csv('/Users/jan-philippviefhues/Desktop/UNI/Maastricht/um/Thesis/data/datasets/cleaned_datasets/hourly_dataset.after.flooding_with_precipitation_and_dummies.csv')\n",
    "dataset_after = dataset_after.rename(columns={'PM25':'PM2.5', 'IsFloodingPeriode': 'Is flooding periode','period': 'Periode','RhineWaterLevel':'Rhine water level',\n",
    "                  'mine_water_level':'Mine water level', 'ground_water_level':'Ground water level','Stream_water_level':'Stream water level','abnormal_Co2_leakage':'CO2 leakage'})\n",
    "\n",
    "best_parameters_for_after_dataset = {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 70, 'bootstrap': True}\n",
    "target_after = 'CO2 leakage' # Can be any kind of data type\n",
    "features_after = ['Is flooding periode','PM2.5', 'PM10', 'Temperature', 'Humidity', 'Pressure', 'Precipitation','Mine water level', 'Ground water level','Rhine water level','Stream water level','Discharge'] # Has to be an array\n",
    "calculate_rf_model(dataset_after, target_after, features_after,best_parameters_for_after_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21429c24",
   "metadata": {},
   "source": [
    "## Optimized model with oversampling for floood prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b09d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "def calculate_rf_model_with_oversampling(dataset, target, features, best_parameters):\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from pprint import pprint\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    " \n",
    "    # Apply oversampling\n",
    "    oversample = RandomOverSampler(sampling_strategy=0.06)\n",
    "    X, y = oversample.fit_resample(X_b, y_b)\n",
    "    X=X.to_numpy()\n",
    "\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11) # 80% training and 20% test\n",
    "    \n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    clf=RandomForestClassifier(n_estimators=best_parameters['n_estimators'],min_samples_split=best_parameters['min_samples_split'],\n",
    "                               min_samples_leaf=best_parameters['min_samples_leaf'], max_features=best_parameters['max_features'], \n",
    "                               max_depth=best_parameters['max_depth'],bootstrap= best_parameters['bootstrap'])\n",
    "     #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred=clf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # Outcome\n",
    "\n",
    "\n",
    "    \n",
    "    #Outcome\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    \n",
    "    # Calculate Feature Importance\n",
    "    featue_names = list(X.columns)\n",
    "    feature_imp = pd.Series(clf.feature_importances_,featue_names).sort_values(ascending=False)\n",
    "    feature_imp\n",
    "    %matplotlib inline\n",
    "    # Creating a bar plot\n",
    "    sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "    # Add labels to your graph\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Visualizing Important Features')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    feature_importances = clf.feature_importances_\n",
    "    sorted_importances = [importance for importance in feature_importances]\n",
    "    sorted_features = [importance for importance in feature_importances]\n",
    "    # Cumulative importances\n",
    "    cumulative_importances = np.cumsum(sorted_importances)\n",
    "    # Make a line graph\n",
    " \n",
    "    #print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)\n",
    "    print(sorted_features)\n",
    "        \n",
    "    \n",
    "    # Score gives accuracy\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    #accuracy = cross_val_score(clf, dataset[features], dataset[target], scoring='accuracy', cv = 10).mean() * 100\n",
    "    \n",
    "    #print(\"Accuracy of Random Forests is: \" , accuracy)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "    y_pred = cross_val_predict(clf, dataset[features], dataset[target], cv=10)\n",
    "\n",
    "    \n",
    "    labels = np.unique(y)\n",
    "    a =  confusion_matrix(y, y_pred, labels=labels)\n",
    "    \n",
    "    print(y_pred)\n",
    "\n",
    "    print(pd.DataFrame(a, index=labels, columns=labels))\n",
    "    \n",
    "\n",
    "    \n",
    "   # precision, recall and F1\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "\n",
    "    recall = cross_val_score(clf, dataset[features], dataset[target], cv=10, scoring='recall')\n",
    "    print('Recall', np.mean(recall), recall)\n",
    "    precision = cross_val_score(clf, dataset[features], dataset[target], cv=10, scoring='precision')\n",
    "    print('Precision', np.mean(precision), precision)\n",
    "    f1 = cross_val_score(clf, dataset[features], dataset[target], cv=10, scoring='f1')\n",
    "    print('F1', np.mean(f1), f1)\n",
    "    \n",
    "    \n",
    "   \n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_parameter_for_flood = {'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': True}\n",
    "calculate_rf_model_with_oversampling(total_dataset, target, features,best_parameter_for_flood)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
